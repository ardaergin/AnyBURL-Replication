{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing datasets and preprocessing them into triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## via torch_geometric\n",
    "\n",
    "Importing `torch_geometric` is just to get the datasets, and will not be used further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Geometric version: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric\n",
    "print(\"PyTorch Geometric version:\", torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knowledge_graph_summary(data, dataset_name):\n",
    "    print(f\"--- {dataset_name} ---\")\n",
    "    print(f\"Number of nodes: {data.num_nodes}\")\n",
    "    print(f\"Number of edges: {data.edge_index.shape[1]}\")\n",
    "    print(f\"Edge index shape: {data.edge_index.shape}\")\n",
    "    print(f\"Edge types shape: {data.edge_type.shape}\")\n",
    "\n",
    "    unique, counts = torch.unique(data.edge_type, return_counts=True)\n",
    "    min_count = torch.min(counts).item()\n",
    "    max_count = torch.max(counts).item()\n",
    "    mean_count = torch.mean(counts.float()).item()\n",
    "    median_count = torch.median(counts.float()).item()\n",
    "\n",
    "    print(f\"Number of unique relation types: {unique.numel()}\")\n",
    "    print(f\"- Minimum relation occurrences: {min_count}\")\n",
    "    print(f\"- Maximum relation occurrences: {max_count}\")\n",
    "    print(f\"- Mean relation occurrences: {mean_count:.2f}\")\n",
    "    print(f\"- Median relation occurrences: {median_count:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_to_triples(edge_index, edge_type):\n",
    "    \"\"\"\n",
    "    Convert PyTorch Geometric tensors to a list of (head, relation, tail) triples.\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): Tensor of shape [2, num_edges] representing (head, tail).\n",
    "        edge_type (torch.Tensor): Tensor of shape [num_edges] representing relation types.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of triples (head, relation, tail).\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        head = edge_index[0, i].item()\n",
    "        tail = edge_index[1, i].item()\n",
    "        relation = edge_type[i].item()\n",
    "        triples.append((head, relation, tail))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WN18 & WN18RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WN18 ---\n",
      "Number of nodes: 40943\n",
      "Number of edges: 151442\n",
      "Edge index shape: torch.Size([2, 151442])\n",
      "Edge types shape: torch.Size([151442])\n",
      "Number of unique relation types: 18\n",
      "- Minimum relation occurrences: 86\n",
      "- Maximum relation occurrences: 37221\n",
      "- Mean relation occurrences: 8413.44\n",
      "- Median relation occurrences: 3150.00\n",
      "--- WN18RR ---\n",
      "Number of nodes: 40943\n",
      "Number of edges: 93003\n",
      "Edge index shape: torch.Size([2, 93003])\n",
      "Edge types shape: torch.Size([93003])\n",
      "Number of unique relation types: 11\n",
      "- Minimum relation occurrences: 86\n",
      "- Maximum relation occurrences: 37221\n",
      "- Mean relation occurrences: 8454.82\n",
      "- Median relation occurrences: 3150.00\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import WordNet18, WordNet18RR\n",
    "\n",
    "WN18 = WordNet18(root='data/WN18')[0]\n",
    "get_knowledge_graph_summary(WN18, \"WN18\")\n",
    "\n",
    "WN18RR = WordNet18RR(root='data/WN18RR')[0]\n",
    "get_knowledge_graph_summary(WN18RR, \"WN18RR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== WN18 =====\n",
      "- Train: 141442 triples\n",
      "- Validation: 5000 triples\n",
      "- Test: 5000 triples\n",
      "\n",
      "Some (train) triples:\n",
      "> (0, 5, 9534)\n",
      "> (0, 15, 12878)\n",
      "> (0, 10, 14747)\n",
      "> (1, 10, 39788)\n",
      "> (1, 2, 40217)\n"
     ]
    }
   ],
   "source": [
    "##### WN18 #####\n",
    "\n",
    "# Accessing predefined splits\n",
    "WN18_mask_train = WN18.train_mask\n",
    "WN18_mask_val = WN18.val_mask\n",
    "WN18_mask_test = WN18.test_mask\n",
    "\n",
    "# Getting train, val, and test splits\n",
    "WN18_edges_train = WN18.edge_index[:, WN18_mask_train]\n",
    "WN18_types_train = WN18.edge_type[WN18_mask_train]\n",
    "\n",
    "WN18_edges_val = WN18.edge_index[:, WN18_mask_val]\n",
    "WN18_types_val = WN18.edge_type[WN18_mask_val]\n",
    "\n",
    "WN18_edges_test = WN18.edge_index[:, WN18_mask_test]\n",
    "WN18_types_test = WN18.edge_type[WN18_mask_test]\n",
    "\n",
    "# Converting to list of triples\n",
    "WN18_triples_train = preprocess_dataset_to_triples(WN18_edges_train, WN18_types_train)\n",
    "WN18_triples_val = preprocess_dataset_to_triples(WN18_edges_val, WN18_types_val)\n",
    "WN18_triples_test = preprocess_dataset_to_triples(WN18_edges_test, WN18_types_test)\n",
    "\n",
    "# Checking split sizes and triples\n",
    "print(\"===== WN18 =====\")\n",
    "print(f\"- Train: {len(WN18_triples_train)} triples\")\n",
    "print(f\"- Validation: {len(WN18_triples_val)} triples\")\n",
    "print(f\"- Test: {len(WN18_triples_test)} triples\")\n",
    "print(\"\\nSome (train) triples:\")\n",
    "print(\"\\n\".join(f\"> {triple}\" for triple in WN18_triples_train[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== WN18RR =====\n",
      "- Train: 86835 triples\n",
      "- Validation: 3034 triples\n",
      "- Test: 3134 triples\n",
      "\n",
      "Some (train) triples:\n",
      "> (0, 3, 10211)\n",
      "> (0, 9, 25525)\n",
      "> (1, 10, 3891)\n",
      "> (1, 1, 5070)\n",
      "> (1, 1, 7723)\n"
     ]
    }
   ],
   "source": [
    "##### WN18RR #####\n",
    "\n",
    "# Accessing predefined splits\n",
    "WN18RR_mask_train = WN18RR.train_mask\n",
    "WN18RR_mask_val = WN18RR.val_mask\n",
    "WN18RR_mask_test = WN18RR.test_mask\n",
    "\n",
    "# Getting train, val, and test splits\n",
    "WN18RR_edges_train = WN18RR.edge_index[:, WN18RR_mask_train]\n",
    "WN18RR_types_train = WN18RR.edge_type[WN18RR_mask_train]\n",
    "\n",
    "WN18RR_edges_val = WN18RR.edge_index[:, WN18RR_mask_val]\n",
    "WN18RR_types_val = WN18RR.edge_type[WN18RR_mask_val]\n",
    "\n",
    "WN18RR_edges_test = WN18RR.edge_index[:, WN18RR_mask_test]\n",
    "WN18RR_types_test = WN18RR.edge_type[WN18RR_mask_test]\n",
    "\n",
    "# Converting to list of triples\n",
    "WN18RR_triples_train = preprocess_dataset_to_triples(WN18RR_edges_train, WN18RR_types_train)\n",
    "WN18RR_triples_val = preprocess_dataset_to_triples(WN18RR_edges_val, WN18RR_types_val)\n",
    "WN18RR_triples_test = preprocess_dataset_to_triples(WN18RR_edges_test, WN18RR_types_test)\n",
    "\n",
    "# Checking split sizes and triples\n",
    "print(\"===== WN18RR =====\")\n",
    "print(f\"- Train: {len(WN18RR_triples_train)} triples\")\n",
    "print(f\"- Validation: {len(WN18RR_triples_val)} triples\")\n",
    "print(f\"- Test: {len(WN18RR_triples_test)} triples\")\n",
    "print(\"\\nSome (train) triples:\")\n",
    "print(\"\\n\".join(f\"> {triple}\" for triple in WN18RR_triples_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FB15k237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FB15k_237 ---\n",
      "Number of nodes: 14541\n",
      "Number of edges: 272115\n",
      "Edge index shape: torch.Size([2, 272115])\n",
      "Edge types shape: torch.Size([272115])\n",
      "Number of unique relation types: 237\n",
      "- Minimum relation occurrences: 37\n",
      "- Maximum relation occurrences: 15989\n",
      "- Mean relation occurrences: 1148.16\n",
      "- Median relation occurrences: 373.00\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import FB15k_237\n",
    "\n",
    "FB15k237 = FB15k_237(root='data/FB15k_237')[0]\n",
    "get_knowledge_graph_summary(FB15k237, \"FB15k_237\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FB15k_237 (Raw) =====\n",
      "- FB15k_237 Train: torch.Size([2, 272115]), torch.Size([272115])\n",
      "- FB15k_237 Validation: torch.Size([2, 17535]), torch.Size([17535])\n",
      "- FB15k_237 Test: torch.Size([2, 20466]), torch.Size([20466])\n",
      "\n",
      "===== FB15k_237 (List) =====\n",
      "- Train: 272115 triples\n",
      "- Validation: 17535 triples\n",
      "- Test: 20466 triples\n",
      "\n",
      "Some (train) triples:\n",
      "> (0, 0, 1)\n",
      "> (2, 1, 3)\n",
      "> (4, 2, 5)\n",
      "> (6, 3, 7)\n",
      "> (8, 4, 9)\n"
     ]
    }
   ],
   "source": [
    "# Loading splits\n",
    "print(\"===== FB15k_237 (Raw) =====\")\n",
    "\n",
    "FB15k237_train = FB15k_237(root='data/FB15k_237', split='train')[0]\n",
    "print(f\"- FB15k_237 Train: {FB15k237_train.edge_index.shape}, {FB15k237_train.edge_type.shape}\")\n",
    "\n",
    "FB15k237_val = FB15k_237(root='data/FB15k_237', split='val')[0]\n",
    "print(f\"- FB15k_237 Validation: {FB15k237_val.edge_index.shape}, {FB15k237_val.edge_type.shape}\")\n",
    "\n",
    "FB15k237_test = FB15k_237(root='data/FB15k_237', split='test')[0]\n",
    "print(f\"- FB15k_237 Test: {FB15k237_test.edge_index.shape}, {FB15k237_test.edge_type.shape}\")\n",
    "\n",
    "# Converting to list of triples\n",
    "FB15k237_triples_train = preprocess_dataset_to_triples(FB15k237_train.edge_index, FB15k237_train.edge_type)\n",
    "FB15k237_triples_val = preprocess_dataset_to_triples(FB15k237_val.edge_index, FB15k237_val.edge_type)\n",
    "FB15k237_triples_test = preprocess_dataset_to_triples(FB15k237_test.edge_index, FB15k237_test.edge_type)\n",
    "\n",
    "# Checking split sizes and triples\n",
    "print(\"\\n===== FB15k_237 (List) =====\")\n",
    "print(f\"- Train: {len(FB15k237_triples_train)} triples\")\n",
    "print(f\"- Validation: {len(FB15k237_triples_val)} triples\")\n",
    "print(f\"- Test: {len(FB15k237_triples_test)} triples\")\n",
    "print(\"\\nSome (train) triples:\")\n",
    "print(\"\\n\".join(f\"> {triple}\" for triple in FB15k237_triples_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was also going to import **FB15k** dataset (alongside FB15k_237), but it does not seem to be available on `torch_geometric`, and even though it is available in HuggingFace, no one seems to be using it after 2019.\n",
    "\n",
    "> \"*The original FB15k dataset suffers from major test leakage through inverse relations, where a large number of test triples could be obtained by inverting triples in the training set. In order to create a dataset without this characteristic, the FB15k_237 describes a subset of FB15k where inverse relations are removed.*\"\n",
    "\n",
    "Hence, I decided to skip the replication for that dataset even though it is present in the 2019 conference publication of AnyBURL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## via HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAGO3-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['head', 'relation', 'tail'],\n",
      "        num_rows: 1079040\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['head', 'relation', 'tail'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['head', 'relation', 'tail'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "YAGO = load_dataset(\"VLyb/YAGO3-10\")\n",
    "print(YAGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== YAGO3-10 =====\n",
      "- Train: 1079040 triples\n",
      "- Validation: 5000 triples\n",
      "- Test: 5000 triples\n",
      "\n",
      "Some (train) triples:\n",
      "> ('Chatou', 'isLocatedIn', 'France')\n",
      "> ('Boo_Young-tae', 'playsFor', 'Yangju_Citizen_FC')\n",
      "> ('Toni_Kuivasto', 'isAffiliatedTo', 'Helsingin_Jalkapalloklubi')\n",
      "> ('Josh_Smith_(soccer)', 'playsFor', 'Trinity_University_(Texas)')\n",
      "> ('Albrecht_DÃ¼rer', 'diedIn', 'Nuremberg')\n"
     ]
    }
   ],
   "source": [
    "# Doing the splits\n",
    "YAGO_train = YAGO[\"train\"]\n",
    "YAGO_triples_train = [tuple(row.values()) for row in YAGO_train]\n",
    "\n",
    "YAGO_val = YAGO[\"validation\"]\n",
    "YAGO_triples_val = [tuple(row.values()) for row in YAGO_val]\n",
    "\n",
    "YAGO_test = YAGO[\"test\"]\n",
    "YAGO_triples_test = [tuple(row.values()) for row in YAGO_test]\n",
    "\n",
    "# Checking split sizes and triples\n",
    "print(\"===== YAGO3-10 =====\")\n",
    "print(f\"- Train: {len(YAGO_triples_train)} triples\")\n",
    "print(f\"- Validation: {len(YAGO_triples_val)} triples\")\n",
    "print(f\"- Test: {len(YAGO_triples_test)} triples\")\n",
    "print(\"\\nSome (train) triples:\")\n",
    "print(\"\\n\".join(f\"> {triple}\" for triple in YAGO_triples_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/triples/WN18_triples_train.pkl\n",
      "Saved: data/triples/WN18_triples_val.pkl\n",
      "Saved: data/triples/WN18_triples_test.pkl\n",
      "Saved: data/triples/WN18RR_triples_train.pkl\n",
      "Saved: data/triples/WN18RR_triples_val.pkl\n",
      "Saved: data/triples/WN18RR_triples_test.pkl\n",
      "Saved: data/triples/FB15k237_triples_train.pkl\n",
      "Saved: data/triples/FB15k237_triples_val.pkl\n",
      "Saved: data/triples/FB15k237_triples_test.pkl\n",
      "Saved: data/triples/YAGO_triples_train.pkl\n",
      "Saved: data/triples/YAGO_triples_val.pkl\n",
      "Saved: data/triples/YAGO_triples_test.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "output_dir = \"data/triples\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def save_if_not_exists(data, filename):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Saved: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File already exists: {file_path}\")\n",
    "\n",
    "# WN18\n",
    "save_if_not_exists(WN18_triples_train, \"WN18_triples_train.pkl\")\n",
    "save_if_not_exists(WN18_triples_val, \"WN18_triples_val.pkl\")\n",
    "save_if_not_exists(WN18_triples_test, \"WN18_triples_test.pkl\")\n",
    "\n",
    "# WN18RR\n",
    "save_if_not_exists(WN18RR_triples_train, \"WN18RR_triples_train.pkl\")\n",
    "save_if_not_exists(WN18RR_triples_val, \"WN18RR_triples_val.pkl\")\n",
    "save_if_not_exists(WN18RR_triples_test, \"WN18RR_triples_test.pkl\")\n",
    "\n",
    "# FB15k-237\n",
    "save_if_not_exists(FB15k237_triples_train, \"FB15k237_triples_train.pkl\")\n",
    "save_if_not_exists(FB15k237_triples_val, \"FB15k237_triples_val.pkl\")\n",
    "save_if_not_exists(FB15k237_triples_test, \"FB15k237_triples_test.pkl\")\n",
    "\n",
    "# YAGO3-10\n",
    "save_if_not_exists(YAGO_triples_train, \"YAGO_triples_train.pkl\")\n",
    "save_if_not_exists(YAGO_triples_val, \"YAGO_triples_val.pkl\")\n",
    "save_if_not_exists(YAGO_triples_test, \"YAGO_triples_test.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
